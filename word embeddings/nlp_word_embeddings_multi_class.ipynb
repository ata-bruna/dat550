{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect claims to fact check in political debates\n",
    "\n",
    "In this project you will implement various classifiers using both neural and feature based technqiues to detect which sentences in political debates should be fact checked.\n",
    "Dataset from ClaimBuster: https://zenodo.org/record/3609356 \n",
    "Evaluate your classifiers using the same metrics as http://ranger.uta.edu/~cli/pubs/2017/claimbuster-kdd17-hassan.pdf (Table 2)\n",
    "\n",
    "Classification report from sklearn provides everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23462 entries, 0 to 23461\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   index       23462 non-null  int64         \n",
      " 1   date        23462 non-null  datetime64[ns]\n",
      " 2   Text        23462 non-null  object        \n",
      " 3   Clean_text  23462 non-null  object        \n",
      " 4   Verdict     23462 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(2), object(2)\n",
      "memory usage: 916.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data_preprocessing/data.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df[\"date\"].dt.year < 2012\n",
    "\n",
    "X_train = df.loc[mask, \"Clean_text\"].values\n",
    "y_train = df.loc[mask, \"Verdict\"].values\n",
    "\n",
    "X_test = df.loc[~mask, \"Clean_text\"].values\n",
    "y_test = df.loc[~mask, \"Verdict\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining vocabulary\n",
    "vocabulary = {}\n",
    "sentences_len = []\n",
    "for sentence in X_train:\n",
    "    for term in sentence.split():\n",
    "        vocabulary.setdefault(term, len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary is composed of 10205 unique words\n"
     ]
    }
   ],
   "source": [
    "# Defining vocabulary size\n",
    "vocabulary_size = list(vocabulary.values())[-1] + 1\n",
    "\n",
    "print(f\"vocabulary is composed of {vocabulary_size} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_encoded = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encodding Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_encoded = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding max sentence length\n",
    "\n",
    "vec_lengths = []\n",
    "for i in X_train_encoded:\n",
    "    vec_lengths.append(len(i))\n",
    "\n",
    "\n",
    "max_length = np.unique(vec_lengths)[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  783   148     0 ...     0     0     0]\n",
      " [  130   110   771 ...     0     0     0]\n",
      " [  462  2841    30 ...     0     0     0]\n",
      " ...\n",
      " [    2  6525    43 ...     0     0     0]\n",
      " [ 1245    49   566 ...     0     0     0]\n",
      " [10205   264     1 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "X_train_embedded=pad_sequences(X_train_encoded,padding='post',maxlen=max_length)\n",
    "print(X_train_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18118, 65)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embedded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5344, 65)\n"
     ]
    }
   ],
   "source": [
    "X_test_embedded=pad_sequences(X_test_encoded,padding='post',maxlen=max_length)\n",
    "print(X_test_embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18118, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_encoder.fit(y_train.reshape(-1, 1))\n",
    "y_encoded = one_hot_encoder.transform(y_train.reshape(-1, 1))\n",
    "\n",
    "y_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5344, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded_test = one_hot_encoder.transform(y_test.reshape(-1,1))\n",
    "y_encoded_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import GlobalMaxPool1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.backend import clear_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history=model.history):\n",
    "    # plot loss during training\n",
    "    plt.subplot(211)\n",
    "    plt.title('Loss')\n",
    "    plt.plot(model_history.history['loss'], label='train')\n",
    "    plt.plot(model_history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    # plot accuracy during training\n",
    "    plt.subplot(212)\n",
    "    plt.title('Accuracy')\n",
    "    plt.plot(model_history.history['accuracy'], label='train')\n",
    "    plt.plot(model_history.history['val_accuracy'], label='test')\n",
    "    plt.legend()\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 65, 81)            826686    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               72800     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 81)                8181      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 246       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 907,913\n",
      "Trainable params: 907,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocabulary_size+1, 81, input_length=max_length))\n",
    "model_lstm.add(LSTM(100))\n",
    "model_lstm.add(Dense(81, activation = \"relu\"))\n",
    "model_lstm.add(Dense(3, activation='softmax'))\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "453/453 [==============================] - 14s 29ms/step - loss: 0.8384 - accuracy: 0.6730 - val_loss: 0.8874 - val_accuracy: 0.6360\n",
      "Epoch 2/5\n",
      "453/453 [==============================] - 13s 28ms/step - loss: 0.8333 - accuracy: 0.6731 - val_loss: 0.8871 - val_accuracy: 0.6360\n",
      "Epoch 3/5\n",
      "453/453 [==============================] - 13s 28ms/step - loss: 0.8325 - accuracy: 0.6731 - val_loss: 0.8977 - val_accuracy: 0.6360\n",
      "Epoch 4/5\n",
      "453/453 [==============================] - 13s 28ms/step - loss: 0.8325 - accuracy: 0.6731 - val_loss: 0.8868 - val_accuracy: 0.6360\n",
      "Epoch 5/5\n",
      "453/453 [==============================] - 13s 28ms/step - loss: 0.8324 - accuracy: 0.6730 - val_loss: 0.8886 - val_accuracy: 0.6360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a8b3669dc0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.fit(X_train_embedded,y_encoded, validation_split=0.2, epochs = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_lstm.predict(X_test_embedded)\n",
    "preds = one_hot_encoder.inverse_transform(predictions).reshape(-1,)\n",
    "print(classification_report(y_test, preds, target_names=[\"NFS\", \"UFS\", \"CFS\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 65, 97)            989982    \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 220)              183040    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 220)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 97)                21437     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 3)                 294       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,194,753\n",
      "Trainable params: 1,194,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_bi = Sequential()\n",
    "model_bi.add(Embedding(vocabulary_size+1, 97, input_length=max_length))\n",
    "model_bi.add(Bidirectional(LSTM(100)))\n",
    "model_bi.add(Dropout(0.5))\n",
    "model_bi.add(Dense(97, activation = \"relu\"))\n",
    "model_bi.add(Dense(3, activation='softmax'))\n",
    "model_bi.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_bi.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "453/453 [==============================] - 21s 41ms/step - loss: 0.6575 - accuracy: 0.7439 - val_loss: 0.6445 - val_accuracy: 0.7409\n",
      "Epoch 2/3\n",
      "453/453 [==============================] - 18s 40ms/step - loss: 0.4717 - accuracy: 0.8239 - val_loss: 0.6823 - val_accuracy: 0.7459\n",
      "Epoch 3/3\n",
      "453/453 [==============================] - 19s 41ms/step - loss: 0.3582 - accuracy: 0.8697 - val_loss: 0.7446 - val_accuracy: 0.7348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a8a9a1c8e0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bi.fit(X_train_embedded,y_encoded, validation_split=0.2, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NFS       0.75      0.89      0.81      3296\n",
      "         UFS       0.39      0.25      0.30       623\n",
      "         CFS       0.64      0.46      0.54      1425\n",
      "\n",
      "    accuracy                           0.70      5344\n",
      "   macro avg       0.59      0.53      0.55      5344\n",
      "weighted avg       0.68      0.70      0.68      5344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model_bi.predict(X_test_embedded)\n",
    "preds = one_hot_encoder.inverse_transform(predictions).reshape(-1,)\n",
    "print(classification_report(y_test, preds, target_names=[\"NFS\", \"UFS\", \"CFS\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 65, 200)           2041200   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 65, 200)           0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 65, 200)          240800    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 200)              240800    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 97)                19497     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 294       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,542,591\n",
      "Trainable params: 2,542,591\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_bi = Sequential()\n",
    "model_bi.add(Embedding(vocabulary_size+1, 200, input_length=max_length))\n",
    "model_bi.add(Dropout(0.2))\n",
    "model_bi.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "model_bi.add(Bidirectional(LSTM(100)))\n",
    "model_bi.add(Dropout(0.2))\n",
    "model_bi.add(Dense(97, activation = \"relu\"))\n",
    "model_bi.add(Dense(3, activation='softmax'))\n",
    "model_bi.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_bi.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "453/453 [==============================] - 52s 107ms/step - loss: 0.6497 - accuracy: 0.7473 - val_loss: 0.6650 - val_accuracy: 0.7312\n",
      "Epoch 2/20\n",
      "453/453 [==============================] - 47s 103ms/step - loss: 0.4797 - accuracy: 0.8170 - val_loss: 0.6686 - val_accuracy: 0.7409\n",
      "Epoch 3/20\n",
      "453/453 [==============================] - 46s 101ms/step - loss: 0.3636 - accuracy: 0.8662 - val_loss: 0.7582 - val_accuracy: 0.7332\n",
      "Epoch 4/20\n",
      "453/453 [==============================] - 47s 103ms/step - loss: 0.2689 - accuracy: 0.9029 - val_loss: 0.8505 - val_accuracy: 0.7243\n",
      "Epoch 5/20\n",
      "453/453 [==============================] - 46s 102ms/step - loss: 0.2005 - accuracy: 0.9295 - val_loss: 1.0763 - val_accuracy: 0.7144\n",
      "Epoch 6/20\n",
      "453/453 [==============================] - 50s 110ms/step - loss: 0.1513 - accuracy: 0.9477 - val_loss: 1.1367 - val_accuracy: 0.7058\n",
      "Epoch 7/20\n",
      "453/453 [==============================] - 50s 110ms/step - loss: 0.1222 - accuracy: 0.9574 - val_loss: 1.2871 - val_accuracy: 0.7014\n",
      "Epoch 8/20\n",
      "453/453 [==============================] - 46s 103ms/step - loss: 0.0931 - accuracy: 0.9658 - val_loss: 1.4548 - val_accuracy: 0.6785\n",
      "Epoch 9/20\n",
      "453/453 [==============================] - 51s 112ms/step - loss: 0.0845 - accuracy: 0.9693 - val_loss: 1.5154 - val_accuracy: 0.6918\n",
      "Epoch 10/20\n",
      "453/453 [==============================] - 48s 106ms/step - loss: 0.0711 - accuracy: 0.9754 - val_loss: 1.6259 - val_accuracy: 0.6915\n",
      "Epoch 11/20\n",
      "453/453 [==============================] - 48s 106ms/step - loss: 0.0627 - accuracy: 0.9775 - val_loss: 1.7256 - val_accuracy: 0.6896\n",
      "Epoch 12/20\n",
      "453/453 [==============================] - 47s 104ms/step - loss: 0.0493 - accuracy: 0.9817 - val_loss: 2.0431 - val_accuracy: 0.6896\n",
      "Epoch 13/20\n",
      "453/453 [==============================] - 47s 104ms/step - loss: 0.0435 - accuracy: 0.9839 - val_loss: 1.9280 - val_accuracy: 0.6799\n",
      "Epoch 14/20\n",
      "453/453 [==============================] - 47s 103ms/step - loss: 0.0414 - accuracy: 0.9850 - val_loss: 2.1827 - val_accuracy: 0.6796\n",
      "Epoch 15/20\n",
      "453/453 [==============================] - 47s 104ms/step - loss: 0.0311 - accuracy: 0.9888 - val_loss: 2.2231 - val_accuracy: 0.6821\n",
      "Epoch 16/20\n",
      "453/453 [==============================] - 47s 103ms/step - loss: 0.0306 - accuracy: 0.9896 - val_loss: 2.2032 - val_accuracy: 0.6802\n",
      "Epoch 17/20\n",
      "453/453 [==============================] - 47s 103ms/step - loss: 0.0261 - accuracy: 0.9904 - val_loss: 2.2394 - val_accuracy: 0.6887\n",
      "Epoch 18/20\n",
      "453/453 [==============================] - 47s 103ms/step - loss: 0.0259 - accuracy: 0.9908 - val_loss: 2.1923 - val_accuracy: 0.6689\n",
      "Epoch 19/20\n",
      "453/453 [==============================] - 47s 104ms/step - loss: 0.0221 - accuracy: 0.9908 - val_loss: 2.4773 - val_accuracy: 0.6832\n",
      "Epoch 20/20\n",
      "453/453 [==============================] - 47s 103ms/step - loss: 0.0228 - accuracy: 0.9924 - val_loss: 2.4754 - val_accuracy: 0.6857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a8cc1399a0>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bi.fit(X_train_embedded,y_encoded, validation_split=0.2, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NFS       0.75      0.91      0.82      3296\n",
      "         UFS       0.36      0.28      0.32       623\n",
      "         CFS       0.68      0.41      0.51      1425\n",
      "\n",
      "    accuracy                           0.70      5344\n",
      "   macro avg       0.60      0.53      0.55      5344\n",
      "weighted avg       0.68      0.70      0.68      5344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model_bi.predict(X_test_embedded)\n",
    "preds = one_hot_encoder.inverse_transform(predictions).reshape(-1,)\n",
    "print(classification_report(y_test, preds, target_names=[\"NFS\", \"UFS\", \"CFS\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 65, 100)           1020600   \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 56, 300)           300300    \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 300)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                19264     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,340,359\n",
      "Trainable params: 1,340,359\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size+1, embedding_dim, input_length=max_length))\n",
    "model.add(Conv1D(128, 10, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "453/453 [==============================] - 8s 16ms/step - loss: 0.6677 - accuracy: 0.7371 - val_loss: 0.6508 - val_accuracy: 0.7343\n",
      "Epoch 2/5\n",
      "453/453 [==============================] - 7s 16ms/step - loss: 0.4292 - accuracy: 0.8388 - val_loss: 0.6923 - val_accuracy: 0.7337\n",
      "Epoch 3/5\n",
      "453/453 [==============================] - 7s 16ms/step - loss: 0.1954 - accuracy: 0.9307 - val_loss: 0.9051 - val_accuracy: 0.7150\n",
      "Epoch 4/5\n",
      "453/453 [==============================] - 7s 16ms/step - loss: 0.0902 - accuracy: 0.9690 - val_loss: 1.2466 - val_accuracy: 0.7183\n",
      "Epoch 5/5\n",
      "453/453 [==============================] - 7s 17ms/step - loss: 0.0441 - accuracy: 0.9868 - val_loss: 1.5655 - val_accuracy: 0.6981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a8b7e95220>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_embedded,y_encoded, validation_split=0.2, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NFS       0.75      0.83      0.79      3296\n",
      "         UFS       0.35      0.16      0.22       623\n",
      "         CFS       0.53      0.53      0.53      1425\n",
      "\n",
      "    accuracy                           0.67      5344\n",
      "   macro avg       0.54      0.51      0.51      5344\n",
      "weighted avg       0.65      0.67      0.65      5344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_embedded)\n",
    "preds = one_hot_encoder.inverse_transform(predictions).reshape(-1,)\n",
    "print(classification_report(y_test, preds, target_names=[\"NFS\", \"UFS\", \"CFS\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural network + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = Sequential()\n",
    "model_conv.add(Embedding(vocabulary_size+1, 100, input_length=max_length))\n",
    "model_conv.add(Dropout(0.2))\n",
    "model_conv.add(Conv1D(100, 8, activation='relu'))\n",
    "model_conv.add(MaxPooling1D(pool_size=10))\n",
    "model_conv.add(LSTM(100))\n",
    "model_conv.add(Dense(32, activation = \"relu\"))\n",
    "model_conv.add(Dense(3, activation='softmax'))\n",
    "model_conv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 65, 100)           1020600   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 65, 100)           0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 58, 100)           80100     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 5, 100)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                3232      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,184,431\n",
      "Trainable params: 1,184,431\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "340/340 [==============================] - 5s 16ms/step - loss: 0.1060 - accuracy: 0.9617 - val_loss: 1.1266 - val_accuracy: 0.7007\n",
      "Epoch 2/5\n",
      "340/340 [==============================] - 5s 16ms/step - loss: 0.0644 - accuracy: 0.9780 - val_loss: 1.3545 - val_accuracy: 0.6995\n",
      "Epoch 3/5\n",
      "340/340 [==============================] - 6s 16ms/step - loss: 0.0511 - accuracy: 0.9823 - val_loss: 1.5313 - val_accuracy: 0.6980\n",
      "Epoch 4/5\n",
      "340/340 [==============================] - 6s 16ms/step - loss: 0.0430 - accuracy: 0.9856 - val_loss: 1.4808 - val_accuracy: 0.7118\n",
      "Epoch 5/5\n",
      "340/340 [==============================] - 5s 16ms/step - loss: 0.0335 - accuracy: 0.9894 - val_loss: 1.9262 - val_accuracy: 0.7003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a881fbceb0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv.fit(X_train_embedded,y_encoded, validation_split=0.4, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NFS       0.74      0.89      0.81      3296\n",
      "         UFS       0.26      0.24      0.25       623\n",
      "         CFS       0.66      0.36      0.46      1425\n",
      "\n",
      "    accuracy                           0.67      5344\n",
      "   macro avg       0.55      0.50      0.51      5344\n",
      "weighted avg       0.66      0.67      0.65      5344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model_conv.predict(X_test_embedded)\n",
    "preds = one_hot_encoder.inverse_transform(predictions).reshape(-1,)\n",
    "print(classification_report(y_test, preds, target_names=[\"NFS\", \"UFS\", \"CFS\"]))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6fac1c46211edb20ff6398462b8c2a023c28966a3a7910175ddefe7a86b617a9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
