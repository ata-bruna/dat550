{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect claims to fact check in political debates\n",
    "\n",
    "In this project you will implement various classifiers using both neural and feature based technqiues to detect which sentences in political debates should be fact checked.\n",
    "Dataset from ClaimBuster: https://zenodo.org/record/3609356 \n",
    "Evaluate your classifiers using the same metrics as http://ranger.uta.edu/~cli/pubs/2017/claimbuster-kdd17-hassan.pdf (Table 2)\n",
    "\n",
    "Classification report from sklearn provides everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import collections\n",
    "import string\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23462 entries, 0 to 23461\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   index       23462 non-null  int64         \n",
      " 1   date        23462 non-null  datetime64[ns]\n",
      " 2   Text        23462 non-null  object        \n",
      " 3   Clean_text  23462 non-null  object        \n",
      " 4   Verdict     23462 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(2), object(2)\n",
      "memory usage: 916.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data_preprocessing/data.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df[\"date\"].dt.year < 2012\n",
    "\n",
    "X_train = df.loc[mask, \"Clean_text\"].values\n",
    "y_train = df.loc[mask, \"Verdict\"].values\n",
    "\n",
    "X_test = df.loc[~mask, \"Clean_text\"].values\n",
    "y_test = df.loc[~mask, \"Verdict\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding using keras - NOT WORKING YET :D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining vocabulary\n",
    "vocabulary = {}\n",
    "sentences_len = []\n",
    "for sentence in X_train:\n",
    "    for term in sentence.split():\n",
    "        vocabulary.setdefault(term, len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary is composed of 10205 unique words\n"
     ]
    }
   ],
   "source": [
    "# Defining vocabulary size\n",
    "vocabulary_size = list(vocabulary.values())[-1] + 1\n",
    "\n",
    "print(f\"vocabulary is composed of {vocabulary_size} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encodding train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_encoded = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encodding Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_encoded = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding max sentence length\n",
    "\n",
    "vec_lengths = []\n",
    "for i in X_train_encoded:\n",
    "    vec_lengths.append(len(i))\n",
    "\n",
    "\n",
    "max_length = np.unique(vec_lengths)[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  783   148     0 ...     0     0     0]\n",
      " [  130   110   771 ...     0     0     0]\n",
      " [  462  2841    30 ...     0     0     0]\n",
      " ...\n",
      " [    2  6525    43 ...     0     0     0]\n",
      " [ 1245    49   566 ...     0     0     0]\n",
      " [10205   264     1 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "X_train_embedded=pad_sequences(X_train_encoded,padding='post',maxlen=max_length)\n",
    "print(X_train_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18118, 65)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embedded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5344, 65)\n"
     ]
    }
   ],
   "source": [
    "X_test_embedded=pad_sequences(X_test_encoded,padding='post',maxlen=max_length)\n",
    "print(X_test_embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18118, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_encoder.fit(y_train.reshape(-1, 1))\n",
    "y_encoded = one_hot_encoder.transform(y_train.reshape(-1, 1))\n",
    "\n",
    "y_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5344, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded_test = one_hot_encoder.transform(y_test.reshape(-1,1))\n",
    "y_encoded_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import GlobalMaxPool1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "import tensorflow.keras.backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = Sequential()\n",
    "model_conv.add(Embedding(vocabulary_size+1, 100, input_length=max_length))\n",
    "model_conv.add(Dropout(0.2))\n",
    "model_conv.add(Conv1D(100, 8, activation='relu'))\n",
    "model_conv.add(MaxPooling1D(pool_size=10))\n",
    "model_conv.add(LSTM(100))\n",
    "model_conv.add(Dense(32, activation = \"relu\"))\n",
    "model_conv.add(Dense(3, activation='softmax'))\n",
    "model_conv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 65, 100)           1020600   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 65, 100)           0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 58, 100)           80100     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 5, 100)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                3232      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,184,431\n",
      "Trainable params: 1,184,431\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "340/340 [==============================] - 8s 19ms/step - loss: 0.7302 - accuracy: 0.7205 - val_loss: 0.6759 - val_accuracy: 0.7380\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 6s 19ms/step - loss: 0.5224 - accuracy: 0.8062 - val_loss: 0.6720 - val_accuracy: 0.7405\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 6s 19ms/step - loss: 0.3755 - accuracy: 0.8562 - val_loss: 0.7717 - val_accuracy: 0.7253\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 6s 19ms/step - loss: 0.2684 - accuracy: 0.8979 - val_loss: 0.8685 - val_accuracy: 0.7156\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 6s 19ms/step - loss: 0.1706 - accuracy: 0.9380 - val_loss: 1.1819 - val_accuracy: 0.7065\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 6s 19ms/step - loss: 0.1051 - accuracy: 0.9644 - val_loss: 1.1098 - val_accuracy: 0.7067\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 7s 20ms/step - loss: 0.0746 - accuracy: 0.9758 - val_loss: 1.3269 - val_accuracy: 0.6951\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 8s 22ms/step - loss: 0.0589 - accuracy: 0.9811 - val_loss: 1.2739 - val_accuracy: 0.6947\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 7s 20ms/step - loss: 0.0437 - accuracy: 0.9856 - val_loss: 1.3913 - val_accuracy: 0.7093\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 7s 19ms/step - loss: 0.0348 - accuracy: 0.9891 - val_loss: 1.5585 - val_accuracy: 0.6952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x252952b45b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv.fit(X_train_embedded,y_encoded, validation_split=0.4, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_conv.predict(X_test_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = one_hot_encoder.inverse_transform(predictions).reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ..., -1, -1, -1], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NFS       0.75      0.87      0.80      3296\n",
      "         UFS       0.28      0.28      0.28       623\n",
      "         CFS       0.65      0.40      0.50      1425\n",
      "\n",
      "    accuracy                           0.68      5344\n",
      "   macro avg       0.56      0.52      0.53      5344\n",
      "weighted avg       0.67      0.68      0.66      5344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds, target_names=[\"NFS\", \"UFS\", \"CFS\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "import tensorflow\n",
    "tensorflow.keras.backend.clear_session()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 65, 100)           1020600   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 61, 128)           64128     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                1290      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,086,051\n",
      "Trainable params: 1,086,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size+1, embedding_dim, input_length=max_length))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "453/453 [==============================] - 7s 14ms/step - loss: 0.6671 - accuracy: 0.7344 - val_loss: 0.6659 - val_accuracy: 0.7252\n",
      "Epoch 2/10\n",
      "453/453 [==============================] - 6s 14ms/step - loss: 0.4321 - accuracy: 0.8363 - val_loss: 0.6801 - val_accuracy: 0.7290\n",
      "Epoch 3/10\n",
      "453/453 [==============================] - 6s 14ms/step - loss: 0.2372 - accuracy: 0.9175 - val_loss: 0.8937 - val_accuracy: 0.7243\n",
      "Epoch 4/10\n",
      "453/453 [==============================] - 6s 14ms/step - loss: 0.1145 - accuracy: 0.9650 - val_loss: 1.0796 - val_accuracy: 0.7081\n",
      "Epoch 5/10\n",
      "453/453 [==============================] - 6s 14ms/step - loss: 0.0599 - accuracy: 0.9817 - val_loss: 1.2710 - val_accuracy: 0.7058\n",
      "Epoch 6/10\n",
      "453/453 [==============================] - 6s 14ms/step - loss: 0.0327 - accuracy: 0.9905 - val_loss: 1.4058 - val_accuracy: 0.7047\n",
      "Epoch 7/10\n",
      "453/453 [==============================] - 6s 14ms/step - loss: 0.0196 - accuracy: 0.9948 - val_loss: 1.6033 - val_accuracy: 0.7086\n",
      "Epoch 8/10\n",
      "453/453 [==============================] - 6s 13ms/step - loss: 0.0143 - accuracy: 0.9952 - val_loss: 1.7020 - val_accuracy: 0.7047\n",
      "Epoch 9/10\n",
      "453/453 [==============================] - 6s 13ms/step - loss: 0.0114 - accuracy: 0.9958 - val_loss: 1.8151 - val_accuracy: 0.6921\n",
      "Epoch 10/10\n",
      "453/453 [==============================] - 6s 13ms/step - loss: 0.0090 - accuracy: 0.9973 - val_loss: 1.9213 - val_accuracy: 0.7012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25298453eb0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_embedded,y_encoded, validation_split=0.2, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b82ba6ee2fa6fc8e732e2ad4d23a9f0e948eca091e73c38d7a866370a3b51fd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
