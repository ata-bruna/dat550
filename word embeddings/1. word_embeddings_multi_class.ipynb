{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors:** \n",
    "- Bruna Atamanczuk (254205) \n",
    "- John Emeka Udegbunam (207951) \n",
    "- Kurt Arve Skipenes Karadas (890802)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect claims to fact check in political debates - Deep learning using word embedding\n",
    "\n",
    "In this project we implement various classifiers using neural networks to detect which sentences in political debates should be fact checked.\n",
    "\n",
    "The following models are implemented: \n",
    "\n",
    "- Bidirectional LSTM\n",
    "- Stacked Bi-LSTM\n",
    "- CNN\n",
    "- CNN + LSTM\n",
    "\n",
    "Dataset from ClaimBuster: https://zenodo.org/record/3609356 \n",
    "The classifiers are evaluated using the same metrics as http://ranger.uta.edu/~cli/pubs/2017/claimbuster-kdd17-hassan.pdf (Table 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23462 entries, 0 to 23461\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   index       23462 non-null  int64         \n",
      " 1   date        23462 non-null  datetime64[ns]\n",
      " 2   Text        23462 non-null  object        \n",
      " 3   Clean_text  23462 non-null  object        \n",
      " 4   Verdict     23462 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(2), object(2)\n",
      "memory usage: 916.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data_preprocessing/data.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df[\"date\"].dt.year < 2012\n",
    "\n",
    "X_train = df.loc[mask, \"Clean_text\"].values\n",
    "y_train = df.loc[mask, \"Verdict\"].values\n",
    "\n",
    "X_test = df.loc[~mask, \"Clean_text\"].values\n",
    "y_test = df.loc[~mask, \"Verdict\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining vocabulary\n",
    "vocabulary = {}\n",
    "sentences_len = []\n",
    "for sentence in X_train:\n",
    "    for term in sentence.split():\n",
    "        vocabulary.setdefault(term, len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary is composed of 10205 unique words\n"
     ]
    }
   ],
   "source": [
    "# Defining vocabulary size\n",
    "vocabulary_size = list(vocabulary.values())[-1] + 1\n",
    "\n",
    "print(f\"vocabulary is composed of {vocabulary_size} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding representation\n",
    "\n",
    "In order to train our models, that text data need to be converted into integers. The text is encoded using the `Tokenizer`. This will return sequences of integers where each number represents is conneced to a dictionary key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_encoded = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encodding Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_encoded = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding max sentence length\n",
    "\n",
    "vec_lengths = []\n",
    "for i in X_train_encoded:\n",
    "    vec_lengths.append(len(i))\n",
    "\n",
    "\n",
    "max_length = np.unique(vec_lengths)[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  783   148     0 ...     0     0     0]\n",
      " [  130   110   771 ...     0     0     0]\n",
      " [  462  2841    30 ...     0     0     0]\n",
      " ...\n",
      " [    2  6525    43 ...     0     0     0]\n",
      " [ 1245    49   566 ...     0     0     0]\n",
      " [10205   264     1 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "X_train_embedded=pad_sequences(X_train_encoded,padding='post',maxlen=max_length)\n",
    "print(X_train_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18118, 65)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embedded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5344, 65)\n"
     ]
    }
   ],
   "source": [
    "X_test_embedded=pad_sequences(X_test_encoded,padding='post',maxlen=max_length)\n",
    "print(X_test_embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18118, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_encoder.fit(y_train.reshape(-1, 1))\n",
    "y_encoded = one_hot_encoder.transform(y_train.reshape(-1, 1))\n",
    "\n",
    "y_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5344, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded_test = one_hot_encoder.transform(y_test.reshape(-1,1))\n",
    "y_encoded_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While creating these models we monitor training loss and validation loss. If the training loss keeps decreasing and the validation loss keeps increasing, it tells us that our model is overfitting and it will not generalize well in new data.\n",
    "One way of avoiding overfitting in deep learning models is to reduce the number of epochs or to set up an EarlyStop. We will use this notebook to check how the models respond, and if necessary a Earlystop will be defined when we perform word embedding using the GloVe model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.backend import clear_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 65, 97)            989982    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 200)              158400    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 97)                19497     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 294       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,168,173\n",
      "Trainable params: 1,168,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_bi = Sequential()\n",
    "model_bi.add(Embedding(vocabulary_size+1, 97, input_length=max_length))\n",
    "model_bi.add(Bidirectional(LSTM(100)))\n",
    "model_bi.add(Dropout(0.5))\n",
    "model_bi.add(Dense(97, activation = \"relu\"))\n",
    "model_bi.add(Dense(3, activation='softmax'))\n",
    "model_bi.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_bi.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "453/453 [==============================] - 24s 48ms/step - loss: 0.6586 - accuracy: 0.7440 - val_loss: 0.6509 - val_accuracy: 0.7486\n",
      "Epoch 2/4\n",
      "453/453 [==============================] - 21s 46ms/step - loss: 0.4658 - accuracy: 0.8252 - val_loss: 0.6639 - val_accuracy: 0.7384\n",
      "Epoch 3/4\n",
      "453/453 [==============================] - 22s 48ms/step - loss: 0.3520 - accuracy: 0.8724 - val_loss: 0.7383 - val_accuracy: 0.7340\n",
      "Epoch 4/4\n",
      "453/453 [==============================] - 22s 49ms/step - loss: 0.2719 - accuracy: 0.9025 - val_loss: 0.8860 - val_accuracy: 0.7254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c3ed3cee50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bi.fit(X_train_embedded,y_encoded, validation_split=0.2, epochs = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NFS       0.73      0.90      0.81      3296\n",
      "         UFS       0.37      0.20      0.26       623\n",
      "         CFS       0.64      0.43      0.51      1425\n",
      "\n",
      "    accuracy                           0.69      5344\n",
      "   macro avg       0.58      0.51      0.53      5344\n",
      "weighted avg       0.66      0.69      0.66      5344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model_bi.predict(X_test_embedded)\n",
    "preds = one_hot_encoder.inverse_transform(predictions).reshape(-1,)\n",
    "print(classification_report(y_test, preds, target_names=[\"NFS\", \"UFS\", \"CFS\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 65, 200)           2041200   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 65, 200)           0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 65, 200)          240800    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 200)              240800    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 97)                19497     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 294       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,542,591\n",
      "Trainable params: 2,542,591\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_bi = Sequential()\n",
    "model_bi.add(Embedding(vocabulary_size+1, 200, input_length=max_length))\n",
    "model_bi.add(Dropout(0.2))\n",
    "model_bi.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "model_bi.add(Bidirectional(LSTM(100)))\n",
    "model_bi.add(Dropout(0.2))\n",
    "model_bi.add(Dense(97, activation = \"relu\"))\n",
    "model_bi.add(Dense(3, activation='softmax'))\n",
    "model_bi.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_bi.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "453/453 [==============================] - 55s 112ms/step - loss: 0.6415 - accuracy: 0.7489 - val_loss: 0.6443 - val_accuracy: 0.7445\n",
      "Epoch 2/5\n",
      "453/453 [==============================] - 50s 110ms/step - loss: 0.4653 - accuracy: 0.8239 - val_loss: 0.6658 - val_accuracy: 0.7414\n",
      "Epoch 3/5\n",
      "453/453 [==============================] - 50s 111ms/step - loss: 0.3538 - accuracy: 0.8713 - val_loss: 0.7189 - val_accuracy: 0.7337\n",
      "Epoch 4/5\n",
      "453/453 [==============================] - 49s 109ms/step - loss: 0.2636 - accuracy: 0.9068 - val_loss: 0.8498 - val_accuracy: 0.7094\n",
      "Epoch 5/5\n",
      "453/453 [==============================] - 51s 112ms/step - loss: 0.1982 - accuracy: 0.9305 - val_loss: 0.9910 - val_accuracy: 0.7092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c3f7f42850>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bi.fit(X_train_embedded,y_encoded, validation_split=0.2, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NFS       0.75      0.85      0.80      3296\n",
      "         UFS       0.33      0.28      0.30       623\n",
      "         CFS       0.60      0.47      0.53      1425\n",
      "\n",
      "    accuracy                           0.68      5344\n",
      "   macro avg       0.56      0.53      0.54      5344\n",
      "weighted avg       0.66      0.68      0.67      5344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model_bi.predict(X_test_embedded)\n",
    "preds = one_hot_encoder.inverse_transform(predictions).reshape(-1,)\n",
    "print(classification_report(y_test, preds, target_names=[\"NFS\", \"UFS\", \"CFS\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 65, 100)           1020600   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 56, 128)           128128    \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,152,955\n",
      "Trainable params: 1,152,955\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size+1, embedding_dim, input_length=max_length))\n",
    "model.add(Conv1D(128, 10, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "453/453 [==============================] - 7s 14ms/step - loss: 0.6709 - accuracy: 0.7358 - val_loss: 0.6447 - val_accuracy: 0.7401\n",
      "Epoch 2/5\n",
      "453/453 [==============================] - 6s 14ms/step - loss: 0.4261 - accuracy: 0.8412 - val_loss: 0.6814 - val_accuracy: 0.7351\n",
      "Epoch 3/5\n",
      "453/453 [==============================] - 7s 14ms/step - loss: 0.2043 - accuracy: 0.9293 - val_loss: 0.8692 - val_accuracy: 0.7150\n",
      "Epoch 4/5\n",
      "453/453 [==============================] - 6s 14ms/step - loss: 0.0932 - accuracy: 0.9707 - val_loss: 1.1766 - val_accuracy: 0.7180\n",
      "Epoch 5/5\n",
      "453/453 [==============================] - 6s 14ms/step - loss: 0.0467 - accuracy: 0.9840 - val_loss: 1.3895 - val_accuracy: 0.7078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c406bb1670>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_embedded,y_encoded, validation_split=0.2, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NFS       0.74      0.87      0.80      3296\n",
      "         UFS       0.33      0.25      0.28       623\n",
      "         CFS       0.60      0.43      0.50      1425\n",
      "\n",
      "    accuracy                           0.68      5344\n",
      "   macro avg       0.56      0.52      0.53      5344\n",
      "weighted avg       0.66      0.68      0.66      5344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_embedded)\n",
    "preds = one_hot_encoder.inverse_transform(predictions).reshape(-1,)\n",
    "print(classification_report(y_test, preds, target_names=[\"NFS\", \"UFS\", \"CFS\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural network + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = Sequential()\n",
    "model_conv.add(Embedding(vocabulary_size+1, 100, input_length=max_length))\n",
    "model_conv.add(Dropout(0.2))\n",
    "model_conv.add(Conv1D(100, 8, activation='relu'))\n",
    "model_conv.add(MaxPooling1D(pool_size=10))\n",
    "model_conv.add(LSTM(100))\n",
    "model_conv.add(Dense(32, activation = \"relu\"))\n",
    "model_conv.add(Dense(3, activation='softmax'))\n",
    "model_conv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 65, 100)           1020600   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 65, 100)           0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 58, 100)           80100     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 5, 100)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                3232      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,184,431\n",
      "Trainable params: 1,184,431\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "453/453 [==============================] - 8s 16ms/step - loss: 0.7042 - accuracy: 0.7295 - val_loss: 0.6845 - val_accuracy: 0.7279\n",
      "Epoch 2/5\n",
      "453/453 [==============================] - 7s 14ms/step - loss: 0.5164 - accuracy: 0.8062 - val_loss: 0.7001 - val_accuracy: 0.7246\n",
      "Epoch 3/5\n",
      "453/453 [==============================] - 7s 15ms/step - loss: 0.3760 - accuracy: 0.8542 - val_loss: 0.8149 - val_accuracy: 0.7103\n",
      "Epoch 4/5\n",
      "453/453 [==============================] - 7s 14ms/step - loss: 0.2588 - accuracy: 0.9000 - val_loss: 1.0283 - val_accuracy: 0.7050\n",
      "Epoch 5/5\n",
      "453/453 [==============================] - 7s 15ms/step - loss: 0.1630 - accuracy: 0.9401 - val_loss: 1.0623 - val_accuracy: 0.7034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c407481760>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv.fit(X_train_embedded,y_encoded, validation_split=0.2, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NFS       0.77      0.84      0.80      3296\n",
      "         UFS       0.29      0.21      0.25       623\n",
      "         CFS       0.56      0.51      0.54      1425\n",
      "\n",
      "    accuracy                           0.68      5344\n",
      "   macro avg       0.54      0.52      0.53      5344\n",
      "weighted avg       0.66      0.68      0.67      5344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model_conv.predict(X_test_embedded)\n",
    "preds = one_hot_encoder.inverse_transform(predictions).reshape(-1,)\n",
    "print(classification_report(y_test, preds, target_names=[\"NFS\", \"UFS\", \"CFS\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the models performed similar or worst than our baseline model. Among the options, both Bidirectional models seemed to perform better then the convolutional models. However the improvement was not so significant to justify using such models over a simpler, less expensive model such as the SVM."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6fac1c46211edb20ff6398462b8c2a023c28966a3a7910175ddefe7a86b617a9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
