{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect claims to fact check in political debates\n",
    "\n",
    "In this project you will implement various classifiers using both neural and feature based technqiues to detect which sentences in political debates should be fact checked.\n",
    "Dataset from ClaimBuster: https://zenodo.org/record/3609356 \n",
    "Evaluate your classifiers using the same metrics as http://ranger.uta.edu/~cli/pubs/2017/claimbuster-kdd17-hassan.pdf (Table 2)\n",
    "\n",
    "Classification report from sklearn provides everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Create advanced model(s) (suggestions are given below)\n",
    "#           -- Generate more features that a model can use. For example the context around the sentence, sentiment, named entities etc.\n",
    "#           -- Rule based classifier. For example, if sentence contains certain words, tags, statistics etc.\n",
    "#           -- Deep learning (word embeddings, transformer models etc.)\n",
    "#           -- Sub-sentence classifier. Long sentences may include several claims, so the goal is to mark the span of claim(s) within a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import collections\n",
    "import string\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and merging the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Speaker_title</th>\n",
       "      <th>Speaker_party</th>\n",
       "      <th>File_id</th>\n",
       "      <th>Length</th>\n",
       "      <th>Line_number</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Verdict</th>\n",
       "      <th>date</th>\n",
       "      <th>mos_before_election</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8211</td>\n",
       "      <td>Now, this is not standing still.</td>\n",
       "      <td>Richard M. Nixon</td>\n",
       "      <td>Vice President</td>\n",
       "      <td>REPUBLICAN</td>\n",
       "      <td>1960-09-26.txt</td>\n",
       "      <td>6</td>\n",
       "      <td>114</td>\n",
       "      <td>-0.417840</td>\n",
       "      <td>-1</td>\n",
       "      <td>1960-09-26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8515</td>\n",
       "      <td>So these are three programs which are quite mo...</td>\n",
       "      <td>John F. Kennedy</td>\n",
       "      <td>Senator</td>\n",
       "      <td>DEMOCRAT</td>\n",
       "      <td>1960-09-26.txt</td>\n",
       "      <td>9</td>\n",
       "      <td>418</td>\n",
       "      <td>0.249581</td>\n",
       "      <td>-1</td>\n",
       "      <td>1960-09-26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8514</td>\n",
       "      <td>The proposal advanced by you and by Mr. Javits...</td>\n",
       "      <td>John F. Kennedy</td>\n",
       "      <td>Senator</td>\n",
       "      <td>DEMOCRAT</td>\n",
       "      <td>1960-09-26.txt</td>\n",
       "      <td>42</td>\n",
       "      <td>417</td>\n",
       "      <td>-0.626563</td>\n",
       "      <td>1</td>\n",
       "      <td>1960-09-26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8513</td>\n",
       "      <td>It does not put a deficit on the Treasury.</td>\n",
       "      <td>John F. Kennedy</td>\n",
       "      <td>Senator</td>\n",
       "      <td>DEMOCRAT</td>\n",
       "      <td>1960-09-26.txt</td>\n",
       "      <td>9</td>\n",
       "      <td>416</td>\n",
       "      <td>-0.629486</td>\n",
       "      <td>1</td>\n",
       "      <td>1960-09-26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8512</td>\n",
       "      <td>The third is medical care for the aged which i...</td>\n",
       "      <td>John F. Kennedy</td>\n",
       "      <td>Senator</td>\n",
       "      <td>DEMOCRAT</td>\n",
       "      <td>1960-09-26.txt</td>\n",
       "      <td>22</td>\n",
       "      <td>415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>1960-09-26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23528</th>\n",
       "      <td>34028</td>\n",
       "      <td>First of all, the media is so dishonest and so...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Businessman</td>\n",
       "      <td>REPUBLICAN</td>\n",
       "      <td>2016-10-19.txt</td>\n",
       "      <td>17</td>\n",
       "      <td>907</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10-19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23529</th>\n",
       "      <td>34027</td>\n",
       "      <td>What I've seen -- what I've seen is so bad.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Businessman</td>\n",
       "      <td>REPUBLICAN</td>\n",
       "      <td>2016-10-19.txt</td>\n",
       "      <td>9</td>\n",
       "      <td>906</td>\n",
       "      <td>-0.669600</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10-19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23530</th>\n",
       "      <td>34026</td>\n",
       "      <td>I'll look at it at the time.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Businessman</td>\n",
       "      <td>REPUBLICAN</td>\n",
       "      <td>2016-10-19.txt</td>\n",
       "      <td>7</td>\n",
       "      <td>905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10-19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23531</th>\n",
       "      <td>34039</td>\n",
       "      <td>So I talk about the corrupt media.</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Businessman</td>\n",
       "      <td>REPUBLICAN</td>\n",
       "      <td>2016-10-19.txt</td>\n",
       "      <td>7</td>\n",
       "      <td>918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10-19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23532</th>\n",
       "      <td>33341</td>\n",
       "      <td>They work hard, they do everything they can to...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>Secretary of State</td>\n",
       "      <td>DEMOCRAT</td>\n",
       "      <td>2016-10-19.txt</td>\n",
       "      <td>14</td>\n",
       "      <td>220</td>\n",
       "      <td>0.361200</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10-19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23533 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence_id                                               Text  \\\n",
       "index                                                                   \n",
       "0             8211                   Now, this is not standing still.   \n",
       "1             8515  So these are three programs which are quite mo...   \n",
       "2             8514  The proposal advanced by you and by Mr. Javits...   \n",
       "3             8513         It does not put a deficit on the Treasury.   \n",
       "4             8512  The third is medical care for the aged which i...   \n",
       "...            ...                                                ...   \n",
       "23528        34028  First of all, the media is so dishonest and so...   \n",
       "23529        34027        What I've seen -- what I've seen is so bad.   \n",
       "23530        34026                       I'll look at it at the time.   \n",
       "23531        34039                 So I talk about the corrupt media.   \n",
       "23532        33341  They work hard, they do everything they can to...   \n",
       "\n",
       "                Speaker       Speaker_title Speaker_party         File_id  \\\n",
       "index                                                                       \n",
       "0      Richard M. Nixon      Vice President    REPUBLICAN  1960-09-26.txt   \n",
       "1       John F. Kennedy             Senator      DEMOCRAT  1960-09-26.txt   \n",
       "2       John F. Kennedy             Senator      DEMOCRAT  1960-09-26.txt   \n",
       "3       John F. Kennedy             Senator      DEMOCRAT  1960-09-26.txt   \n",
       "4       John F. Kennedy             Senator      DEMOCRAT  1960-09-26.txt   \n",
       "...                 ...                 ...           ...             ...   \n",
       "23528      Donald Trump         Businessman    REPUBLICAN  2016-10-19.txt   \n",
       "23529      Donald Trump         Businessman    REPUBLICAN  2016-10-19.txt   \n",
       "23530      Donald Trump         Businessman    REPUBLICAN  2016-10-19.txt   \n",
       "23531      Donald Trump         Businessman    REPUBLICAN  2016-10-19.txt   \n",
       "23532   Hillary Clinton  Secretary of State      DEMOCRAT  2016-10-19.txt   \n",
       "\n",
       "       Length  Line_number  Sentiment  Verdict       date  mos_before_election  \n",
       "index                                                                           \n",
       "0           6          114  -0.417840       -1 1960-09-26                    2  \n",
       "1           9          418   0.249581       -1 1960-09-26                    2  \n",
       "2          42          417  -0.626563        1 1960-09-26                    2  \n",
       "3           9          416  -0.629486        1 1960-09-26                    2  \n",
       "4          22          415   0.000000       -1 1960-09-26                    2  \n",
       "...       ...          ...        ...      ...        ...                  ...  \n",
       "23528      17          907   0.032300       -1 2016-10-19                    1  \n",
       "23529       9          906  -0.669600       -1 2016-10-19                    1  \n",
       "23530       7          905   0.000000       -1 2016-10-19                    1  \n",
       "23531       7          918   0.000000       -1 2016-10-19                    1  \n",
       "23532      14          220   0.361200       -1 2016-10-19                    1  \n",
       "\n",
       "[23533 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1 = pd.read_csv(\"data/crowdsourced.csv\", encoding='utf-8')\n",
    "file2 = pd.read_csv(\"data/groundtruth.csv\", encoding='utf-8')\n",
    "df = pd.concat([file1, file2])\n",
    "\n",
    "\n",
    "df[\"date\"] = df[\"File_id\"].str.strip(to_strip=\".txt\")\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df.sort_values(\"date\", inplace= True)\n",
    "df[\"mos_before_election\"] = 11 - df[\"date\"].dt.month\n",
    "\n",
    "df['index'] = pd.RangeIndex(len(df))\n",
    "df.set_index('index', inplace=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    tokens = re.sub('[^a-zA-Z]', ' ', text).lower()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    word_list = [word for word in text.split() if word not in stop_words]\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_stemm(word_list):\n",
    "    \"\"\"Stemmers remove morphological affixes from words, leaving only the word stem.\"\"\"\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    singles = [stemmer.stem(word) for word in word_list] \n",
    "    return singles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(docs):\n",
    "\n",
    "    text_list = [] \n",
    "    for doc in docs:  \n",
    "        # 1. Remove punctuation and set as lower case\n",
    "        text = remove_punctuation(doc)\n",
    "\n",
    "        # 2. Remove stop words and extra spaces\n",
    "        word_list = remove_stop_words(text)\n",
    "        joined_text = \" \".join(word_list)\n",
    "        text_list.append(joined_text)\n",
    "        \n",
    "        # 3. Stemming\n",
    "        # word_stem = get_word_stemm(word_list)\n",
    "        # joined_text = \" \".join(word_stem)\n",
    "        # text_list.append(joined_text)\n",
    "\n",
    "\n",
    "    return text_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_data(df.Text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add clean text to dataframe\n",
    "df[\"Clean_text\"] = pd.Series(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del file1, file2, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Speaker_title</th>\n",
       "      <th>Speaker_party</th>\n",
       "      <th>File_id</th>\n",
       "      <th>Length</th>\n",
       "      <th>Line_number</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Verdict</th>\n",
       "      <th>date</th>\n",
       "      <th>mos_before_election</th>\n",
       "      <th>Clean_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8211</td>\n",
       "      <td>Now, this is not standing still.</td>\n",
       "      <td>Richard M. Nixon</td>\n",
       "      <td>Vice President</td>\n",
       "      <td>REPUBLICAN</td>\n",
       "      <td>1960-09-26.txt</td>\n",
       "      <td>6</td>\n",
       "      <td>114</td>\n",
       "      <td>-0.417840</td>\n",
       "      <td>-1</td>\n",
       "      <td>1960-09-26</td>\n",
       "      <td>2</td>\n",
       "      <td>standing still</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8515</td>\n",
       "      <td>So these are three programs which are quite mo...</td>\n",
       "      <td>John F. Kennedy</td>\n",
       "      <td>Senator</td>\n",
       "      <td>DEMOCRAT</td>\n",
       "      <td>1960-09-26.txt</td>\n",
       "      <td>9</td>\n",
       "      <td>418</td>\n",
       "      <td>0.249581</td>\n",
       "      <td>-1</td>\n",
       "      <td>1960-09-26</td>\n",
       "      <td>2</td>\n",
       "      <td>three programs quite moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8514</td>\n",
       "      <td>The proposal advanced by you and by Mr. Javits...</td>\n",
       "      <td>John F. Kennedy</td>\n",
       "      <td>Senator</td>\n",
       "      <td>DEMOCRAT</td>\n",
       "      <td>1960-09-26.txt</td>\n",
       "      <td>42</td>\n",
       "      <td>417</td>\n",
       "      <td>-0.626563</td>\n",
       "      <td>1</td>\n",
       "      <td>1960-09-26</td>\n",
       "      <td>2</td>\n",
       "      <td>proposal advanced mr javits would cost six hun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8513</td>\n",
       "      <td>It does not put a deficit on the Treasury.</td>\n",
       "      <td>John F. Kennedy</td>\n",
       "      <td>Senator</td>\n",
       "      <td>DEMOCRAT</td>\n",
       "      <td>1960-09-26.txt</td>\n",
       "      <td>9</td>\n",
       "      <td>416</td>\n",
       "      <td>-0.629486</td>\n",
       "      <td>1</td>\n",
       "      <td>1960-09-26</td>\n",
       "      <td>2</td>\n",
       "      <td>put deficit treasury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8512</td>\n",
       "      <td>The third is medical care for the aged which i...</td>\n",
       "      <td>John F. Kennedy</td>\n",
       "      <td>Senator</td>\n",
       "      <td>DEMOCRAT</td>\n",
       "      <td>1960-09-26.txt</td>\n",
       "      <td>22</td>\n",
       "      <td>415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>1960-09-26</td>\n",
       "      <td>2</td>\n",
       "      <td>third medical care aged tied social security f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence_id                                               Text  \\\n",
       "index                                                                   \n",
       "0             8211                   Now, this is not standing still.   \n",
       "1             8515  So these are three programs which are quite mo...   \n",
       "2             8514  The proposal advanced by you and by Mr. Javits...   \n",
       "3             8513         It does not put a deficit on the Treasury.   \n",
       "4             8512  The third is medical care for the aged which i...   \n",
       "\n",
       "                Speaker   Speaker_title Speaker_party         File_id  Length  \\\n",
       "index                                                                           \n",
       "0      Richard M. Nixon  Vice President    REPUBLICAN  1960-09-26.txt       6   \n",
       "1       John F. Kennedy         Senator      DEMOCRAT  1960-09-26.txt       9   \n",
       "2       John F. Kennedy         Senator      DEMOCRAT  1960-09-26.txt      42   \n",
       "3       John F. Kennedy         Senator      DEMOCRAT  1960-09-26.txt       9   \n",
       "4       John F. Kennedy         Senator      DEMOCRAT  1960-09-26.txt      22   \n",
       "\n",
       "       Line_number  Sentiment  Verdict       date  mos_before_election  \\\n",
       "index                                                                    \n",
       "0              114  -0.417840       -1 1960-09-26                    2   \n",
       "1              418   0.249581       -1 1960-09-26                    2   \n",
       "2              417  -0.626563        1 1960-09-26                    2   \n",
       "3              416  -0.629486        1 1960-09-26                    2   \n",
       "4              415   0.000000       -1 1960-09-26                    2   \n",
       "\n",
       "                                              Clean_text  \n",
       "index                                                     \n",
       "0                                         standing still  \n",
       "1                          three programs quite moderate  \n",
       "2      proposal advanced mr javits would cost six hun...  \n",
       "3                                   put deficit treasury  \n",
       "4      third medical care aged tied social security f...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df[\"date\"].dt.year < 2012\n",
    "\n",
    "X_train = df.loc[mask, \"Clean_text\"].values\n",
    "y_train = df.loc[mask, \"Verdict\"].values\n",
    "\n",
    "X_test = df.loc[~mask, \"Clean_text\"].values\n",
    "y_test = df.loc[~mask, \"Verdict\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.6, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base line model\n",
    "\n",
    "1. SVM\n",
    "2. KNN\n",
    "3. Perceptron\n",
    "4. Naive Bayes\n",
    "5. Decision Tree \n",
    "5. Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import time \n",
    "\n",
    "def baseline(clf):\n",
    "    print(\"_\" * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time.time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time.time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time.time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    print()\n",
    "    print(\"                       Classification report\")\n",
    "    report = classification_report(y_test, pred, target_names= [\"NFS\", \"UFS\", \"CFS\"])\n",
    "    print(report)\n",
    "\n",
    "    return {\n",
    "            \"score\": score,\n",
    "            \"report\": report, \n",
    "            \"train_time\": train_time, \n",
    "            \"test_time\": test_time, \n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results = {}\n",
    "\n",
    "clfs = [\n",
    "        (\"SVM\", svm.SVC(kernel='linear')), \n",
    "        (\"KNN\", KNeighborsClassifier(n_neighbors=10)), \n",
    "        (\"Perceptron\", Perceptron(max_iter=50)),\n",
    "        (\"NB\", ComplementNB()),\n",
    "        (\"DT\", DecisionTreeClassifier()),\n",
    "        (\"RF\", RandomForestClassifier(min_samples_split=5))\n",
    "        ]   \n",
    "\n",
    "for classifier in clfs:\n",
    "    result = baseline(classifier[1])  \n",
    "    results.setdefault(classifier[0], result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding using keras - NOT WORKING YET :D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining vocabulary size\n",
    "vocabulary_size = list(unique_word_dict.values())[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoded_vocab = [one_hot(words, vocabulary_size) for words in df[\"Text_clean\"].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finding max sentence length\n",
    "\n",
    "vec_lengths = []\n",
    "for i in encoded_vocab:\n",
    "    vec_lengths.append(len(i))\n",
    "\n",
    "\n",
    "max_length = np.unique(vec_lengths)[-1]\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedded_docs=pad_sequences(encoded_vocab,padding='pre',maxlen=max_length)\n",
    "print(embedded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size,30,input_length=max_length))\n",
    "model.compile('adam','mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.predict_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kurt's try - Classification with a BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e7049d",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/tokenizers/python/latest/components.html#models <br>\n",
    "_One of the most popular subword tokenization algorithm. The Byte-Pair-Encoding works by starting with characters, while merging those that are the most frequently seen together, thus creating new tokens. It then works iteratively to build new tokens out of the most frequent pairs it sees in a corpus.BPE is able to build words it has never seen by using multiple subword tokens, and thus requires smaller vocabularies, with less chances of having “unk” (unknown) tokens._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f60c6",
   "metadata": {},
   "source": [
    "James briggs, https://youtu.be/GYDFBfx8Ts8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before encoding, we need to figure out how long each sequence to be because the encoder method also acts like a padding or truncation method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FYI: raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x18c4a99e8e0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seqlen2 = df[\"Clean_text\"].apply(lambda x: len(x.split()))\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize = (12,10))\n",
    "sns.displot(seqlen2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3467740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen = pd.DataFrame(X_train, columns=[\"Clean_text\"])[\"Clean_text\"].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "182e6d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x18c4d452280>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.displot(seqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f748103",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize tokenizer from a pretrained model. Since we have lowercase text, we use the uncased BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c0c02cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 4.65kB/s]\n",
      "Downloading: 100%|██████████| 570/570 [00:00<?, ?B/s] \n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 496kB/s] \n",
      "Downloading: 100%|██████████| 455k/455k [00:00<00:00, 1.08MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have initiated the tokenizer, we can do the encoding. add each sequence into a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0701715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# initializing np arrays, length of df by the sequence length that is defined above\n",
    "\n",
    "Xids = np.zeros((len(X_train), SEQ_LEN))\n",
    "Xmask = np.zeros((len(X_train), SEQ_LEN))\n",
    "\n",
    "for i, text in enumerate(X_train):\n",
    "    tokens = tokenizer.encode_plus(text, \n",
    "        max_length = SEQ_LEN, \n",
    "        truncation = True, \n",
    "        padding = \"max_length\",\n",
    "        add_special_tokens = True, \n",
    "        return_token_type_ids = False, \n",
    "        return_attention_mask = True, \n",
    "        return_tensors = \"tf\")\n",
    "    Xids[i, :] = tokens[\"input_ids\"]\n",
    "    Xmask[i, :] = tokens[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef9c8121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101.,  3061.,  2145., ...,     0.,     0.,     0.],\n",
       "       [  101.,  2093.,  3454., ...,     0.,     0.,     0.],\n",
       "       [  101.,  6378.,  3935., ...,     0.,     0.,     0.],\n",
       "       ...,\n",
       "       [  101.,  2228.,  4012., ...,     0.,     0.,     0.],\n",
       "       [  101., 19821.,  2488., ...,     0.,     0.,     0.],\n",
       "       [  101., 22395.,  2283., ...,     0.,     0.,     0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c47469f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xmask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xids_test = np.zeros((len(X_test), SEQ_LEN))\n",
    "Xmask_test = np.zeros((len(X_test), SEQ_LEN))\n",
    "\n",
    "for i, text in enumerate(X_test):\n",
    "    tokens = tokenizer.encode_plus(text, \n",
    "        max_length = SEQ_LEN, \n",
    "        truncation = True, \n",
    "        padding = \"max_length\",\n",
    "        add_special_tokens = True, \n",
    "        return_token_type_ids = False, \n",
    "        return_attention_mask = True, \n",
    "        return_tensors = \"tf\")\n",
    "    Xids_test[i, :] = tokens[\"input_ids\"]\n",
    "    Xmask_test[i, :] = tokens[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea40ae8",
   "metadata": {},
   "source": [
    "one-hot-encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3657a9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18170, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.zeros((y_train.size, len(np.unique(y_train))))\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f1c6c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.arange(y_train.size), y_train] = 1\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternatively, using sk-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_encoder.fit(y_train.reshape(-1, 1))\n",
    "y_encoded = one_hot_encoder.transform(y_train.reshape(-1, 1))\n",
    "y_encoded = pd.DataFrame(data=y_encoded, columns=one_hot_encoder.categories_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test = np.zeros((y_test.size, len(np.unique(y_test))))\n",
    "\n",
    "labels_test[np.arange(y_test.size), y_test] = 1\n",
    "labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee314d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"xids.npy\", \"wb\") as f:\n",
    "    np.save(f, Xids)\n",
    "with open(\"xmasks.npy\", \"wb\") as f:\n",
    "    np.save(f, Xmask)\n",
    "with open(\"labels.npy\", \"wb\") as f:\n",
    "    np.save(f, labels)\n",
    "with open(\"labels2.npy\", \"wb\") as f:\n",
    "    np.save(f, y_encoded)\n",
    "\n",
    "\n",
    "# delete from memory    \n",
    "# del df, Xids, Xmask, labels, y_encoded, X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1ccf66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"xids.npy\", \"rb\") as fp:\n",
    "    Xids = np.load(fp)\n",
    "with open(\"xmasks.npy\", \"rb\") as fp:\n",
    "    Xmask = np.load(fp)\n",
    "with open(\"labels.npy\", \"rb\") as fp:\n",
    "    labels = np.load(fp)\n",
    "with open(\"labels2.npy\", \"rb\") as fp:\n",
    "    y_encoded = np.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "369a0520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow dataset object: makes it a lot easier (restructure the data, shuffle and batch it in just a few lines of code) - faster in terms of performance and faster for us to code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > restructure the data:\n",
    "\n",
    " tensorflow expect the data to be input as a tuple consisting of our input and target label. since we are using BERT, our data should be slightly different. input tuple  should be a dictionary containing an key that is input_id = Xids and attention_mask = Xmask. 1) create dataset object ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17cd6627",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, y_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = tf.data.Dataset.from_tensor_slices((Xids_test, Xmask_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50f8092b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 101., 3061., 2145.,  102.,    0.,    0.,    0.,    0.,    0.,\n",
      "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
      "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
      "          0.,    0.,    0.])>, <tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>, <tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n"
     ]
    }
   ],
   "source": [
    "for i in dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above returns three tensors. tensorflow expect that the data in is a tuple format. 0 index needs to be our input and the 1 index of that tuple will be our labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7548b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_fn(input_ids, masks, y_encoded):# labels):\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": masks}, y_encoded #labels\n",
    "\n",
    "dataset = dataset.map(map_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = dataset_test.map(map_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee706b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([ 101., 3061., 2145.,  102.,    0.,    0.,    0.,    0.,    0.,\n",
      "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
      "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
      "          0.,    0.,    0.])>, 'attention_mask': <tf.Tensor: shape=(30,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>}, <tf.Tensor: shape=(3,), dtype=float64, numpy=array([0., 0., 1.])>)\n"
     ]
    }
   ],
   "source": [
    "for i in dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f244ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling and batching data is easy:\n",
    "dataset = dataset.shuffle(100_000).batch(32)\n",
    "dataset_test = dataset_test.shuffle(100_000).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c31d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_LEN = len(list(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "250b879a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DS_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbb928b",
   "metadata": {},
   "source": [
    "SPLIT = 0.9\n",
    "\n",
    "train = dataset.take(round(DS_LEN * SPLIT))\n",
    "val = dataset.skip(round(DS_LEN * SPLIT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7f54d0",
   "metadata": {},
   "source": [
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2c6706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 511M/511M [00:16<00:00, 33.1MB/s] \n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "bert = TFAutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to build a network around bert. first define input layers (there are two Xids and Xmask). name is superimportant that is matching the naming from the dict above. otherwise it will fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac63a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our input layers:\n",
    "input_ids = tf.keras.layers.Input(shape = (SEQ_LEN,), name = \"input_ids\", dtype=\"int32\") # input data type important\n",
    "mask = tf.keras.layers.Input(shape = (SEQ_LEN,), name = \"attention_mask\", dtype=\"int32\")\n",
    "\n",
    "# bert consumes our two input layers\n",
    "# and return two tensors to us, latency (3d) and ola/cuda? output (2d)\n",
    "embeddings = bert(input_ids, attention_mask = mask)[0]\n",
    "\n",
    "# to keep it simple, we use a global max pooling layer. but could experiment with lstm layers, convolutional layers, or anything else\n",
    "X = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
    "\n",
    "# could have skipped the step above and change index 0 to 1 to get the 2d layer - the global max pool layer will convert our 3d output tensor to a 2d tensor\n",
    "\n",
    "X = tf.keras.layers.BatchNormalization()(X) # normalizing the output - usually leads to better results\n",
    "# following a densly connected neural net  which are responsible for classification of our bert output\n",
    "X = tf.keras.layers.Dense(128, activation = \"relu\")(X)\n",
    "X = tf.keras.layers.Dropout(0.1)(X) #prevents overfitting, or too much overfitting\n",
    "X = tf.keras.layers.Dense(32, activation = \"relu\")(X)\n",
    "# output layer\n",
    "y = tf.keras.layers.Dense(3, activation = \"softmax\", name = \"outputs\")(X)\n",
    "\n",
    "# the above is the model architecture\n",
    "# also, we need to tell TensorFlow what our input layers are and what is our output:\n",
    "model = tf.keras.Model(inputs = [input_ids, mask], outputs = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91cfb529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 30,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 768)         3072        ['global_max_pooling1d[0][0]']   \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          98432       ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           4128        ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 3)            99          ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,587,971\n",
      "Trainable params: 109,586,435\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6028a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# almost all parameters are trainable. \n",
    "# don't want to train Bert - we're freezing the bert layer:\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 30,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 768)         3072        ['global_max_pooling1d[0][0]']   \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          98432       ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           4128        ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 3)            99          ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,587,971\n",
      "Trainable params: 104,195\n",
      "Non-trainable params: 109,483,776\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can now put together our optimizer, loss and accuracy, compile model and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7937a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.05) # learning rate of 0.01 - originally\n",
    "loss = tf.keras.losses.CategoricalCrossentropy() # due to one hot encoding\n",
    "acc = tf.keras.metrics.CategoricalAccuracy(\"accuracy\") # same as above\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = loss, metrics = [acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can train our model <br>\n",
    "<font color = \"red\"> needs much more epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8cc45558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "568/568 [==============================] - 600s 1s/step - loss: 0.8706 - accuracy: 0.6641 - val_loss: 0.9090 - val_accuracy: 0.6179\n",
      "Epoch 2/5\n",
      "568/568 [==============================] - 497s 875ms/step - loss: 0.8449 - accuracy: 0.6660 - val_loss: 0.9054 - val_accuracy: 0.6179\n",
      "Epoch 3/5\n",
      "568/568 [==============================] - 500s 881ms/step - loss: 0.8455 - accuracy: 0.6660 - val_loss: 0.9050 - val_accuracy: 0.6179\n",
      "Epoch 4/5\n",
      "568/568 [==============================] - 545s 959ms/step - loss: 0.8453 - accuracy: 0.6660 - val_loss: 0.9049 - val_accuracy: 0.6179\n",
      "Epoch 5/5\n",
      "568/568 [==============================] - 572s 1s/step - loss: 0.8447 - accuracy: 0.6660 - val_loss: 0.9005 - val_accuracy: 0.6179\n"
     ]
    }
   ],
   "source": [
    "# history = model.fit(train, validation_data= val, epochs = 1)\n",
    "history = model.fit(dataset, validation_data= dataset_test, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c47c40c",
   "metadata": {},
   "source": [
    "model.save(\"my_model\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b82ba6ee2fa6fc8e732e2ad4d23a9f0e948eca091e73c38d7a866370a3b51fd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
