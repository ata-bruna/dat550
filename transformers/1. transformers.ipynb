{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect claims to fact check in political debates\n",
    "\n",
    "In this project you will implement BERT model to detect which sentences in political debates should be fact checked.\n",
    "Dataset from ClaimBuster: https://zenodo.org/record/3609356\n",
    "\n",
    "The classifier is evalued using the same metrics as http://ranger.uta.edu/~cli/pubs/2017/claimbuster-kdd17-hassan.pdf (Table 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23462 entries, 0 to 23461\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   index       23462 non-null  int64         \n",
      " 1   date        23462 non-null  datetime64[ns]\n",
      " 2   Text        23462 non-null  object        \n",
      " 3   Clean_text  23462 non-null  object        \n",
      " 4   Verdict     23462 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(2), object(2)\n",
      "memory usage: 916.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data_preprocessing/data.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df[\"date\"].dt.year < 2012\n",
    "\n",
    "X_train = df.loc[mask, \"Clean_text\"].values\n",
    "y_train = df.loc[mask, \"Verdict\"].values\n",
    "\n",
    "X_test = df.loc[~mask, \"Clean_text\"].values\n",
    "y_test = df.loc[~mask, \"Verdict\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5344,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TRANSFORMER IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encoding the y_train data\n",
    "\n",
    "labels = np.zeros((y_train.size, len(np.unique(y_train))))\n",
    "labels[np.arange(y_train.size), y_train] = 1\n",
    "\n",
    "y_encoded_train = labels\n",
    "y_encoded_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encoding the y_test data\n",
    "\n",
    "labels_test = np.zeros((y_test.size, len(np.unique(y_test))))\n",
    "labels_test[np.arange(y_test.size), y_test] = 1\n",
    "\n",
    "y_encoded_test = labels_test\n",
    "y_encoded_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded_test[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a Transformer block as a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load BERT Model and Tokenizer from the transformers package \n",
    "\n",
    "- The **tokenizer** converts the input text into tokens.\n",
    "\n",
    "- The **classAutoTokenizer** contains various types of tokenizers.\n",
    "\n",
    "- **TFBertModel** pre-trained **Bert** model for TensorFlow.\n",
    "\n",
    "- The **bert-base-cased** model is used in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,TFBertModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "bert = TFBertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['standing', 'still']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Token = tokenizer.tokenize(X_train[0])\n",
    "Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data Modeling\n",
    "\n",
    "- Before training the deep learning model, the input text data are converted into BERT’s input data format using the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input data using tokenizer (bert-base-cased)\n",
    "\n",
    "x_train = tokenizer(\n",
    "    text=X_train.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=50,\n",
    "    truncation=True,\n",
    "    padding=True, \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)\n",
    "\n",
    "\n",
    "x_test = tokenizer(\n",
    "    text=X_test.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=50,\n",
    "    truncation=True,\n",
    "    padding=True, \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(5344, 50), dtype=int32, numpy=\n",
       "array([[  101,  2215,  1143, ...,     0,     0,     0],\n",
       "       [  101,  2905,   102, ...,     0,     0,     0],\n",
       "       [  101, 17847,  3136, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  1440,  1159, ...,     0,     0,     0],\n",
       "       [  101,  2037, 14644, ...,     0,     0,     0],\n",
       "       [  101,  1250,  1662, ...,     0,     0,     0]])>, 'attention_mask': <tf.Tensor: shape=(5344, 50), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **tokenizer** takes the necessary parameters and returns tensor in the same format Bert accepts.\n",
    "- **return_token_type_ids** is set **False** because **token_type_ids** is not required for our task.\n",
    "- **return_attention_mask** is **True** means attention_mask is included in the input.\n",
    "- **return_tensors=’tf’** implies input tensor for the TensorFlow model.\n",
    "- **max_length=50** implies the maximum length of each sentence is 50. Sentences with bigger lengths will be trimmed to 50, while smaller sentence lengths will be padded to 50.\n",
    "- **add_special_tokens=True** means **CLS, SEP** token will be added in the tokenization.\n",
    "\n",
    "As outputs of the  data modeling, the **tokenizer** will return a dictionary (x_train) containing **‘Input_ids’**, **‘attention_mask’** as key for their respective data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(18118, 50), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = x_train['input_ids']\n",
    "attention_mask = x_train['attention_mask']\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(18118, 50), dtype=int32, numpy=\n",
       "array([[  101,  2288,  1253, ...,     0,     0,     0],\n",
       "       [  101,  1210,  2648, ...,     0,     0,     0],\n",
       "       [  101,  5835,  3682, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  1341,  3254, ...,     0,     0,     0],\n",
       "       [  101, 21744,  1618, ...,     0,     0,     0],\n",
       "       [  101, 25092,  1710, ...,     0,     0,     0]])>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5344, 50), dtype=int32, numpy=\n",
       "array([[  101,  2215,  1143, ...,     0,     0,     0],\n",
       "       [  101,  2905,   102, ...,     0,     0,     0],\n",
       "       [  101, 17847,  3136, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  1440,  1159, ...,     0,     0,     0],\n",
       "       [  101,  2037, 14644, ...,     0,     0,     0],\n",
       "       [  101,  1250,  1662, ...,     0,     0,     0]])>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(18118, 50), dtype=int32, numpy=\n",
       "array([[  101,  2288,  1253, ...,     0,     0,     0],\n",
       "       [  101,  1210,  2648, ...,     0,     0,     0],\n",
       "       [  101,  5835,  3682, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  1341,  3254, ...,     0,     0,     0],\n",
       "       [  101, 21744,  1618, ...,     0,     0,     0],\n",
       "       [  101, 25092,  1710, ...,     0,     0,     0]])>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "\n",
    "Importing necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Keras functional API is used to design the deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "embeddings = bert(input_ids,attention_mask = input_mask)[0] \n",
    "out = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
    "out = Dense(128, activation='relu')(out)\n",
    "out = tf.keras.layers.Dropout(0.1)(out)\n",
    "out = Dense(32,activation = 'relu')(out)\n",
    "y = Dense(3,activation = 'softmax')(out)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n",
    "\n",
    "model.layers[2].trainable = True\n",
    "#model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Bert layers** accept three input arrays: **input_ids**, **attention_mask**, and **token_type_ids**\n",
    "\n",
    "- **input_ids** contains input word encoding.\n",
    "\n",
    "- **token_type_ids** is required for the question-answering model and hence will not be used here.\n",
    "\n",
    "- For the Bert layer,**input_ids** and **attention_mask** are used as two input layers.\n",
    "- **Embeddings** contain hidden states of the Bert layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 50,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          98432       ['global_max_pooling1d[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           4128        ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 3)            99          ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,412,931\n",
      "Trainable params: 108,412,931\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Compilation\n",
    "\n",
    "**Define learning parameters and compile the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=5e-05, epsilon=1e-08,decay=0.01,clipnorm=1.0)\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**learning_rate = 5e-05**: the learning rate for the model will be significantly lower.\n",
    "\n",
    "**Loss = CategoricalCrossentropy**: the target has 3 unique integer values representing two distinct categories for the multi-class classification task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "Train the model with the training tensors, while the test tensors are used for the model validation.\n",
    "\n",
    "Training and fine-tuning of the BERT model takes a bit longer time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "504/504 [==============================] - 6859s 14s/step - loss: 0.5702 - accuracy: 0.7766 - val_loss: 0.6296 - val_accuracy: 0.7481\n"
     ]
    }
   ],
   "source": [
    "train_history = model.fit(\n",
    "    x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n",
    "    y = y_encoded_train,\n",
    "    validation_data = (\n",
    "    {'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}, y_encoded_test\n",
    "    ),\n",
    "  epochs=1,\n",
    "    batch_size=36\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Model Evaluation**\n",
    "\n",
    "Testing the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_raw = model.predict({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10026944, 0.05610476, 0.8436258 ],\n",
       "       [0.36755702, 0.28952134, 0.34292173],\n",
       "       [0.27910215, 0.08186944, 0.63902843],\n",
       "       ...,\n",
       "       [0.06776952, 0.03152383, 0.90070665],\n",
       "       [0.1288251 , 0.06344972, 0.8077252 ],\n",
       "       [0.04614429, 0.02721756, 0.9266382 ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04799846, 0.83002764, 0.12197381], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_raw[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df.loc[~mask, \"Verdict\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the prediction probabilities to the target classes \n",
    "\n",
    "y_predicted = []\n",
    "\n",
    "for t in predicted_raw:\n",
    "    #print(t)\n",
    "    x = np.argmax(t)\n",
    "    x2 = 0\n",
    "    if x == 2:      #  Class 3: NFS\n",
    "        x2 = -1    \n",
    "    elif x ==1:     # Class 2: CFS\n",
    "        x2 = 1\n",
    "    elif x ==0:     # Class 1: UFS\n",
    "        x2 = 0\n",
    "    y_predicted.append(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yt: 1 yp: 1 yr: [0.04799846 0.83002764 0.12197381] yc: 1\n"
     ]
    }
   ],
   "source": [
    "yp = y_predicted[30]\n",
    "yt = y_true[30]\n",
    "yr = predicted_raw[30]\n",
    "yc =np.argmax(predicted_raw[30])\n",
    "\n",
    "print('yt:', yt, 'yp:',yp,'yr:',yr,'yc:',yc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array(y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NFS       0.78      0.91      0.84      3296\n",
      "         UFS       0.47      0.41      0.44       623\n",
      "         CFS       0.76      0.52      0.61      1425\n",
      "\n",
      "    accuracy                           0.75      5344\n",
      "   macro avg       0.67      0.61      0.63      5344\n",
      "weighted avg       0.74      0.75      0.74      5344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_true, y_predicted, target_names= [\"NFS\", \"UFS\", \"CFS\"])\n",
    "\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd00500f8d60e23badaae0e7928590fc2dced0bf2434cb09173f5d5d219b11a4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
